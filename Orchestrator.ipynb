{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f259c61c-e1ec-4f1f-844d-a53a9218757a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "7d98e1ad-e69d-4ed3-841a-c7a3d6ce901b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install -U langchain-databricks langchain-core langchain mlflow langgraph\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ed98c4f-8b0d-4bd5-80fb-e4b90dfb745e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e07b563-9634-4292-a7fa-dfd17c6bd749",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "from typing import TypedDict, Annotated, Sequence\n",
    "from langchain_core.messages import BaseMessage, HumanMessage\n",
    "from langchain_databricks import ChatDatabricks\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "@tool\n",
    "def get_user_balance(user_id: int) -> str:\n",
    "  \"\"\"\n",
    "  Finds the account balance for a specific user_id from the user_data table.\n",
    "  \"\"\"\n",
    "  print(f\"--- Tool Called: get_user_balance(user_id={user_id}) ---\")\n",
    "  try:\n",
    "    df = spark.sql(f\"SELECT account_balance FROM main.agents_demo.user_data WHERE user_id = {user_id}\")\n",
    "    result = df.first()\n",
    "    if result:\n",
    "      return f\"The balance for user_id {user_id} is ${result['account_balance']}\"\n",
    "    else:\n",
    "      return f\"No user found with user_id {user_id}\"\n",
    "  except Exception as e:\n",
    "    return f\"Error querying database: {str(e)}\"\n",
    "\n",
    "tools = [get_user_balance]\n",
    "\n",
    "# --- Define the Agent's State ---\n",
    "# This is the \"memory\" of our graph.\n",
    "class AgentState(TypedDict):\n",
    "    # 'messages' will hold the list of all messages in the conversation\n",
    "    messages: Annotated[Sequence[BaseMessage], operator.add]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "13968055-2fae-40f7-aa77-fdadc54cba8b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Graph Nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b9e5b8de-4859-4a77-a01d-524fb6c9cb89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Define the Graph Edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4477c29-ab73-4a4e-98c1-16943b7a4fe5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- Imports from Step 2 (make sure you have these) ---\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, ToolMessage, AIMessage\n",
    "import operator\n",
    "from typing import TypedDict, Annotated, Sequence\n",
    "from langchain_databricks import ChatDatabricks\n",
    "# --- Imports for our NEW Step 3 ---\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "# --- Your LLM and Tools (from Step 1 & 2) ---\n",
    "llm = ChatDatabricks(endpoint=\"databricks-gpt-oss-120b\", max_tokens=200)\n",
    "\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "# Create a much simpler prompt. \n",
    "# It just takes the whole list of messages.\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant. You must use your tools to answer questions.\"),\n",
    "    MessagesPlaceholder(variable_name=\"messages\"), # This placeholder holds all chat history\n",
    "])\n",
    "\n",
    "# Our agent runnable is now a simple chain\n",
    "agent_runnable = prompt | llm_with_tools\n",
    "\n",
    "# --- 2. Define the 'call_model' node ---\n",
    "# This function is now incredibly simple.\n",
    "def call_model(state: AgentState):\n",
    "    \"\"\"Our \"Planner\" node. It calls the LLM.\"\"\"\n",
    "    print(\"--- Calling Model ---\")\n",
    "    \n",
    "    # The 'agent_runnable' takes the entire state (which is just {\"messages\": [...]})\n",
    "    # It returns a single AIMessage, either with content or with tool_calls.\n",
    "    response_message = agent_runnable.invoke(state)\n",
    "    \n",
    "    # We just add this new message to the list of messages\n",
    "    return {\"messages\": [response_message]}\n",
    "\n",
    "# --- 3. Define the 'call_tool' node ---\n",
    "# but include it here for completeness)\n",
    "def call_tool(state: AgentState):\n",
    "    \"\"\"Our \"Tool Executor\" node.\"\"\"\n",
    "    print(\"--- Calling Tool ---\")\n",
    "    \n",
    "    last_message = state['messages'][-1] # Get the AIMessage with tool calls\n",
    "    \n",
    "    tool_outputs = []\n",
    "    for tool_call in last_message.tool_calls:\n",
    "        tool_name = tool_call['name']\n",
    "        tool_args = tool_call['args']\n",
    "        \n",
    "        # Find the actual tool function from our list\n",
    "        selected_tool = next(t for t in tools if t.name == tool_name)\n",
    "        \n",
    "        # Call the tool\n",
    "        print(f\"--- Calling Tool: {tool_name} with args {tool_args} ---\")\n",
    "        output = selected_tool.invoke(tool_args)\n",
    "        \n",
    "        # Store the output in a ToolMessage\n",
    "        tool_outputs.append(\n",
    "            ToolMessage(content=str(output), tool_call_id=tool_call['id'])\n",
    "        )\n",
    "\n",
    "    # Return the tool outputs to be added to the state\n",
    "    return {\"messages\": tool_outputs}\n",
    "# --- 4. Define the conditional edge ---\n",
    "def should_continue(state: AgentState):\n",
    "    \"\"\"\n",
    "    This function decides our next step.\n",
    "    - If the last message has tool calls, we go to the 'call_tool' node.\n",
    "    - Otherwise, we 'END' the graph.\n",
    "    \"\"\"\n",
    "    last_message = state['messages'][-1]\n",
    "    \n",
    "    # Check if the last message is from the AI and has tool calls\n",
    "    if isinstance(last_message, AIMessage) and last_message.tool_calls:\n",
    "        return \"continue_to_tools\"\n",
    "    else:\n",
    "        return \"end_conversation\"\n",
    "\n",
    "# --- 5. Build the Graph ---\n",
    "# NOW you can run the code that uses 'should_continue'\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# Add nodes...\n",
    "workflow.add_node(\"agent\", call_model)\n",
    "workflow.add_node(\"action\", call_tool)\n",
    "\n",
    "# Set entry point...\n",
    "workflow.set_entry_point(\"agent\")\n",
    "\n",
    "# Add the conditional edge...\n",
    "workflow.add_conditional_edges(\n",
    "    \"agent\",          # The node to branch from\n",
    "    should_continue,  # <-- This function MUST be defined first\n",
    "    {\n",
    "        \"continue_to_tools\": \"action\", \n",
    "        \"end_conversation\": END\n",
    "    }\n",
    ")\n",
    "\n",
    "# Add remaining edges...\n",
    "workflow.add_edge(\"action\", \"agent\")\n",
    "\n",
    "# Compile...\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bdf91bb9-c2b7-43be-bf0e-02053add79f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Run The agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "626d73db-ada5-4a25-b15d-da6ee3aa5dfe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Loggin with MLflow\n",
    "import mlflow\n",
    "mlflow.set_experiment(\"/Users/ebrahemelsherif666i@gmail.com/langgraph_agent_demo\")\n",
    "mlflow.langchain.autolog()\n",
    "\n",
    "with mlflow.start_run(run_name=\"LangGraph Agent Run\"):\n",
    "    \n",
    "    # Note: The input to a graph is a list of messages\n",
    "    inputs = {\"messages\": [HumanMessage(content=\"What is the balance for user_id 3?\")]}\n",
    "    \n",
    "    # 'stream' the output to see every step\n",
    "    for output in app.stream(inputs):\n",
    "        # 'output' will be a dictionary with the name of the node that just ran\n",
    "        print(f\"--- Step: {list(output.keys())[0]} ---\")\n",
    "        print(output[list(output.keys())[0]])\n",
    "        print(\"\\n\")\n",
    "\n",
    "# To get just the final response\n",
    "# final_response = app.invoke(inputs)\n",
    "# print(final_response['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "129a19e6-02aa-4d27-9a49-e52a22939654",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8468372673522486,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Orchestrator",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
